\clearpage
\item \points{23} {\bf Neural Networks: MNIST image classification}

In this problem, you will implement a simple neural network
to classify grayscale images of handwritten digits (0 - 9) from
the MNIST dataset. The dataset contains 60,000 training images and
10,000 testing images of handwritten digits, 0 - 9. Each image is
28$\times$28 pixels in size, and is generally represented as a flat
vector of 784 numbers. It also includes labels for each example, a number
indicating the actual digit (0 - 9) handwritten in that image. A sample of
a few such images along with their labels are shown below.

% TODO: Find nn-mnist-sample if we use this question
% \begin{center}
% \includegraphics[scale=0.6]{nn-mnist-sample}
% \end{center}

The data for this problem can be found in the data folder as \texttt{images\_train.csv}, \texttt{images\_test.csv}, \texttt{labels\_train.csv} and \texttt{labels\_test.csv}.


The code for this assignment can be found within \texttt{p1\_nn.py} within the src folder.

The starter code splits the set
of 60,000 training images and labels into a sets of 50,000 examples as
the training set and 10,000 examples for dev set.

To start, you will implement a neural network with a single hidden layer
and cross entropy loss, and train it with the provided data set. Use the
sigmoid function as activation for the hidden layer, and softmax function
for the output layer. Recall that for a single example $(x, y)$, the cross
entropy loss is:
$$CE(y, \hat{y}) = - \sum_{k=1}^K y_k \log \hat{y_k},$$
where $\hat{y} \in \mathbb{R}^{K}$ is the vector of softmax outputs
from the model for the training example $x$,
and $y \in \mathbb{R}^{K}$ is the ground-truth vector for the training example
$x$ such that $y = [0,...,0,1,0,...,0]^\top$ contains a single 1 at the
position of the correct class (also called a ``one-hot'' representation).

For $m$ training examples, we average the cross entropy loss over the $m$ examples.
  \begin{equation*}
  J(W^{[1]},W^{[2]},b^{[1]},b^{[2]}) = \frac{1}{m}\sum_{i=1}^m CE(y^{(i)}, \hat{y}^{(i)}) = - \frac{1}{m}\sum_{i=1}^m \sum_{k=1}^K y_k^{(i)} \log \hat{y}_k^{(i)}.
  \end{equation*}
The starter code already converts labels into one hot representations for you.

Instead of batch gradient descent or stochastic gradient descent, common practice
is to use mini-batch gradient descent for deep learning tasks. In this case, the
cost function is defined as follows:

  \begin{equation*}
  J_{MB} = \frac{1}{B}\sum_{i=1}^{B}CE(y^{(i)}, \hat{y}^{(i)})
  \end{equation*}
where $B$ is the batch size, i.e. the number of training example in each mini-batch. 

\begin{enumerate}
\item \points{15}

Implement the following functions within \texttt{p1\_nn.py}. We recommend that you start at the top of the list and work your way down:

\begin{enumerate}
\item \texttt{softmax}
\item \texttt{sigmoid}
\item \texttt{get\_initial\_params} 
\item \texttt{forward\_prop}
\item \texttt{backward\_prop}

\end{enumerate}

Please see the comments for particular instructions for each function.

\item \points{5} Now add a regularization term to your cross entropy loss.
The loss function will become \begin{equation*}
  J_{MB} = \left(\frac{1}{B}\sum_{i=1}^{B}CE(y^{(i)}, \hat{y}^{(i)})\right) + \lambda \left(||W^{[1]}||^2 + ||W^{[2]}||^2 \right)
  \end{equation*}

Be careful not to regularize the bias/intercept term (remember why? from PS2?).
Implement \texttt{backward\_prop\_regularized} that computes the backward step for this loss function.

\item \points{3}

Run \texttt{p1\_nn.py} in order to train and test both regularized and unregularized nn models. Plots of the accuracy and loss on the training and dev sets are stored in \texttt{output/baseline.pdf} and \texttt{output/regularized.pdf} for the unregularized and regularized models respectively.

The accuracy on the test set is printed out on the command line.

Compare the performance of the regularized and unregularized neural network model. Which one has better test accuracy? Is the better test accuracy due to lower bias or lower variance? Please back up your answer with information from the generated plots.

\textbf{Hint:} Be sure to vectorize your code as much as possible! Training can be
very slow otherwise.

\end{enumerate}


\input{01-nn-mnist/00-main-sol.tex}
  
