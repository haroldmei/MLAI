{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS221, Spring 2019, PS2 Reconstruct\n",
    "Haiyuan Mei (hmei0411@stanford.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: word segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Show that the greedy search is suboptimal\n",
    "* The following example input string on greedy approach fails to find the lowest-cost segmentation of the input (generated by unigramCost function). \n",
    " Example: whatisyouname  \n",
    " Should be 'what is your name' in UCS algorithm, cost for each words are with UCS algorithm:  \n",
    "   * what 5.47877310482  \n",
    "   * is 5.08040259845  \n",
    "   * your 6.46401383481  \n",
    "   * name 8.32568529323  \n",
    "   totalCost = 25.34887483131\n",
    "  \n",
    "  But the greedy algorithm ends up with: what i s you r name.  \n",
    "   * what 5.47877310482  \n",
    "   * i 4.83525677784  \n",
    "   * s 5.08040259845  \n",
    "   * you 5.00015324078  \n",
    "   * r 6.46401383481  \n",
    "   * name 8.32568529323  \n",
    "   totalCost = 35.18428484993\n",
    "   \n",
    "* The reason is because in the whole search tree, the greedy approach is completely ignorance of future costs; it makes decision merely according to the next state. Mathematically speaking, suppose $j = \\arg \\min \\ell(s_{0-j})$ is the index that gives the minimum cost in all sub strings of s, starting from index 0 to j but exclude index j, n is the total length of s; and the unigram cost is denoted as $\\ell_{uni}(w)$:\n",
    "$$\\begin{eqnarray*}\n",
    "\\ell(s) &=& \\min_i(\\ell_{uni}(s_{0-i}+\\ell(s_{i-n}))  \\\\\n",
    "&\\le& \\ell_{uni}(s_{0-j})+\\ell(s_{j-n})\n",
    "\\end{eqnarray*}$$\n",
    "The above tells us that even the first sub string gives the least cost, it cannot guarrantee the cost of total cost of the whole string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Coding of the state-space search problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: vowel insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Show that the greedy search is suboptimal\n",
    "* The example below shows that the greedy approach fails to find the lowest-cost vowel insertion (cost function generated by bigramModel in makeLanguageModels).  \n",
    " Example: ['ths', 'ppl', 'wrkd', 'hpply']  \n",
    " Should be 'these people worked happily', cost for each bigram with UCS algorithm:  \n",
    "    * (-BEGIN, these) 9.23381883315  \n",
    "    * (these, people) 10.0867919595  \n",
    "    * (people, worked) 13.3055145899  \n",
    "    * (worked, happily) 13.304704934  \n",
    "    totalCost = 45.93083031655\n",
    " \n",
    " But the greedy approach ends up with: this appeal worked happily  \n",
    "    * (-BEGIN-, this) 7.88331357988  \n",
    "    * (this, appeal) 13.3083299497  \n",
    "    * (appeal, worked) 13.3047082673  \n",
    "    * (worked, happily) 13.304704934  \n",
    "    totalCost = 47.80105673088  \n",
    "  \n",
    "* mathematical explaination. Suppose the vowel free string list is $s$, previous word is denoted as $p$, the current vowel free string index is $i$, and the possible fills for $s_i$ is list $f_i$; and suppose $k$ is the index of $f_i$ such that $f_i[k]$ gives the minimum next bigram cost. The bigram cost is denoted as $\\ell_{bi}(w1,w2)$ and the cost function satisfies the following: \n",
    "$$\\begin{eqnarray*}\n",
    "\\ell(p, s_i) &=& \\min_j (\\ell_{bi}(p, f_i[j]) + \\ell(f_i[j], s_{i+1})) \\\\\n",
    "&\\le& \\ell_{bi}(p, f_i[k]) + \\ell(f_i[k], s_{i+1})\n",
    "\\end{eqnarray*}$$\n",
    "The above inquality explains that the greedy approach cannot supass the UCS algorithm, it can only reach suboptimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: putting it together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Find a minimal representation of the states.\n",
    "Suppose the current remaining string is $s$, $i$ is a index such that $s$[:i] is used for possibleFills to generate possible words.\n",
    "* States: (prevWord, $s$[i:]), prevWord is one of possibleFills($s$[:i]), and $s$[i:] is the remaining vowel free space free sub string, $1\\le i \\lt len(s)$\n",
    "* $s_{start}$: (wordsegUtil.SENTENCE_BEGIN, $s$[0:]) tuple.  \n",
    "* Actions($s$): one word chosen from possibleFills($s$[:i]), $1\\le i \\lt len(s)$\n",
    "* Cost($s$; $a$): bigramCost(prevWord, $a$), $a$ is chosen from possibleFills($s$[:i]), $1\\le i \\lt len(s)$.\n",
    "* Succ($s$; $a$): ($a$, $s$[i:]), $a$ is chosen from possibleFills($s$[:i]), $1\\le i \\lt len(s)$\n",
    "* IsEnd($s$): $s$== ''?  \n",
    "\n",
    "Since bigram cost needs to have two consequent words as input, the states should keep track of previous word and current remaining string in order to compare different costs, thus the above (prevWord, $s$[i:]) is the minimal representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Speed up joint space and vowel insertion with A*\n",
    "For the relaxed problem:  \n",
    "* We can apply 'Easier Search' trick as introduced in class, simplify the state space to include only the current remaining string. That way the state space only contains n elements; \n",
    "* The actions will become the index i used to retrive next word $w$ from 'possibleFills($s$[:i])'.\n",
    "* The cost for each state is is simply $u_b(w) = \\min_{i}(b(w_i, w))$, here $w$ is chosen from 'possibleFills($s$[:i])'\n",
    "* The end state is just an empty string.\n",
    "\n",
    "The modified cost of relaxed problem $u_b(w) = \\min_{i}(b(w_i, w))$ basically says, \n",
    "$$\\begin{eqnarray*}\n",
    "\\text{textrmCost}_{rel}(s,a) &=& u_b(w) \\\\\n",
    "&=& \\min_{i}(b(w_i, w))  \\\\\n",
    "&\\le& b(w', w) \\\\\n",
    "&=& \\text{textrmCost}(s,a)\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Now prove the consistency of the relaxed heuristic $h(s)$:\n",
    "$$\\begin{eqnarray*}\n",
    "h(s)&=&\\text{textrmFutureCost}_{rel}(s) \\\\\n",
    "&\\le& \\text{textrmCost}_{rel}(s,a)+h(\\text{Succ}(s,a)) \\\\\n",
    "&\\le& \\text{textrmCost}(s,a)+h(\\text{Succ}(s,a))\n",
    "\\end{eqnarray*}$$\n",
    "The first $\\le$ is because of triangle inequality, the second $\\le$ is from relaxation; the above says the heuristic is consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Relationship between different search algorithms\n",
    "* Is UCS a special case of A*? \n",
    "    The answer is yes, UCS is a special case of A* with h(s) = 0.  \n",
    "    \n",
    "* Is BFS a special case of UCS? \n",
    "    BFS requires the cost of each edge to be constant. In this sense BFS behaves the same as UCS and can be considered a special case of UCS. The difference is that since BFS assumes constant edge cost, it uses an FIFO queue as opposed to a priority queue used by UCS.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
