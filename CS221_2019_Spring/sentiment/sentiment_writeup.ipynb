{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS221, Spring 2019, PS2 Sentiment\n",
    "Haiyuan Mei (hmei0411@stanford.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a\n",
    "* The gradient of hinge loss w.r.t. $\\mathbf{w}$ is \n",
    "  $$\n",
    "    \\nabla_\\mathbf{w} \\text{Loss}_{\\text{hinge}}(x, y, \\mathbf{w}) = \\begin{cases}\n",
    "    -\\phi(x) y,  \\mathbf{w} \\cdot \\phi(x) y \\le 1 \\\\\n",
    "    0, \\text{otherwise} \\\\\n",
    "    \\end{cases}\n",
    "  $$\n",
    "  \n",
    "*  Represent the 4 samples with vectors in the order of (\"pretty\", \"good\", \"bad\", \"plot\", \"not\", \"scenery\"), and their scores/margins starting from $\\mathbf{w}=\\vec{0}$: \n",
    "\n",
    "| - |  x | y | old w | score | margin | gradient | new w |\n",
    "|---|---------|---|---|-------|--------|----------|--------|\n",
    "| 1.|  [1,0,1,0,0,0] | -1 | [0,0,0,0,0,0] | score=0 | margin=0 | [1,0,1,0,0,0]  | [-0.5,0,-0.5,0,0,0] |\n",
    "| 2.|  [0,1,0,1,0,0] | +1 | [-0.5,0,-0.5,0,0,0] | score=0 | margin=0 | [0,-1,0,-1,0,0] | [-0.5,0.5,-0.5,0.5,0,0] |\n",
    "| 3.|  [0,1,0,0,1,0] | -1 | [-0.5,0.5,-0.5,0.5,0,0] | score=0.5 | margin=-0.5 | [0,1,0,0,1,0] | [-0.5,0,-0.5,0.5,-0.5,0] |\n",
    "| 4.|  [1,0,0,0,0,1] | +1 | [-0.5,0,-0.5,0.5,-0.5,0] | score=-0.5 | margin=-0.5 | [-1,0,0,0,0,-1] | [0,0,-0.5,0.5,-0.5,0.5] |\n",
    "\n",
    "* Conclusion: after the 4 samples trained, the weights for each of the six words are: $[0,0,-0.5,0.5,-0.5,0.5]$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b\n",
    "Prove that no linear classifier can gain 0 error. Here the feature vector reuses the above word features (6 component in the review vector)\n",
    "* The 4 new datasets are:\n",
    "  1. ($-1$) not good:  \n",
    "    $x^{(1)}=[0,1,0,0,1,0]$, $y^{(1)}=-1$\n",
    "    \n",
    "  2. ($+1$) good:  \n",
    "    $x^{(1)}=[0,1,0,0,0,0]$, $y^{(1)}=+1$\n",
    "    \n",
    "  3. ($+1$) not bad:  \n",
    "    $x^{(1)}=[0,0,1,0,1,0]$, $y^{(1)}=+1$\n",
    "    \n",
    "  4. ($-1$) bad:  \n",
    "    $x^{(1)}=[0,0,1,0,0,0]$, $y^{(1)}=-1$\n",
    "    \n",
    " If there exists a $\\mathbf{w}$ such that it makes no error, then the following should be true:\n",
    " $$\n",
    " \\begin{cases}\n",
    " w_1 + w_4 < 0 \\\\\n",
    " w_1 > 0 \\\\\n",
    " w_2 + w_4 > 0 \\\\\n",
    " w_2 < 0\n",
    " \\end{cases}\n",
    " $$\n",
    " This is impossible, because: if $w_1 > 0$, we must have $w_4<0$; since $w_2 < 0$, then $w_2 + w_4 > 0$ is impossible.  \n",
    "\n",
    "\n",
    "* Add an additional feature that could fix the problem, we could add a feature the number of words in the review. Suppose it is the 7th component in the review feature vector; the 4 datasets can be denoted as:\n",
    "  1. ($-1$) not good:  \n",
    "    $x^{(1)}=[0,1,0,0,1,0,2]$, $y^{(1)}=-1$\n",
    "    \n",
    "  2. ($+1$) good:  \n",
    "    $x^{(1)}=[0,1,0,0,0,0,1]$, $y^{(1)}=+1$\n",
    "    \n",
    "  3. ($+1$) not bad:  \n",
    "    $x^{(1)}=[0,0,1,0,1,0,2]$, $y^{(1)}=+1$\n",
    "    \n",
    "  4. ($-1$) bad:  \n",
    "    $x^{(1)}=[0,0,1,0,0,0,1]$, $y^{(1)}=-1$\n",
    "  \n",
    "  we can find a new $\\mathbf w$ which makes no error on the dataset by making:\n",
    " $$\n",
    " \\begin{cases}\n",
    " w_1 + w_4 + 2 w_6< 0 \\\\\n",
    " w_1 + w_6 > 0 \\\\\n",
    " w_2 + w_4 + 2 w_6> 0 \\\\\n",
    " w_2 + w_6< 0\n",
    " \\end{cases}\n",
    " $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a\n",
    "The Loss expression can be written as:\n",
    "$$\\begin{eqnarray*}\n",
    "\\text{Loss}(x, y, \\mathbf w) &=& (y - \\sigma(z))^2 \\\\\n",
    " &=& (y - \\sigma(\\mathbf w \\cdot \\phi(x)))^2\\\\\n",
    " &=& \\left ( y - \\frac{1}{1+e^{-\\mathbf w \\cdot \\phi(x)}}\\right ) ^2\n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b\n",
    "By applying the chain rule of gradient, the gradient w.r.t. $\\mathbf w$:  \n",
    "$$\n",
    "\\nabla_\\mathbf{w} \\text{Loss}(x, y, \\mathbf w) = -2 (y - \\sigma(z))\\sigma(z)(1-\\sigma(z))\\phi(x) \n",
    "$$\n",
    "$\\text{where } z = \\mathbf w \\cdot \\phi(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.C\n",
    "From section b, replace y by 1, think of $||\\phi(x)||$ as some constant and consider the following function (here for the ease of description, the sigmoid function is just denoted by a variable $\\sigma$):\n",
    "$$\\begin{eqnarray*}\n",
    "f(\\sigma) &=& \\lVert -2 (y - \\sigma(z))\\sigma(z)(1-\\sigma(z))\\phi(x) \\rVert \\\\\n",
    "&=& 2 \\lVert \\phi(x) \\rVert (\\sigma-1)^2\\sigma , \\text{where } \\sigma \\in (0,1)\n",
    "\\end{eqnarray*}$$\n",
    "The function looks like following:\n",
    "![image](./image.png)\n",
    "\n",
    "This function touches 0 twice when $\\sigma=0$ or $\\sigma=1$, but $\\sigma$ can never reach 0 or 1, so $f(\\sigma)$ can not reach a  minimum, it is lower bounded by 0, and it can never be 0. Consider the sigmoid function $\\sigma(\\mathbf z) = \\sigma(\\mathbf w \\cdot \\phi(x))$, the convergence will be very slow because: \n",
    "$$\n",
    "\\lim_{\\lVert \\mathbf w \\rVert \\longrightarrow \\infty} \\sigma(\\mathbf w \\cdot \\phi(x)) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.d \n",
    "When $\\sigma \\in (0,1)$, it can reach maximun when it's gradient is 0:\n",
    "$$\\begin{eqnarray*}\n",
    "f'(\\sigma) &=& -4 (\\sigma-1)\\sigma - 2(\\sigma-1)^2 \\\\\n",
    "&=& -6\\sigma^2 + 8\\sigma - 2 = 0 \\\\\n",
    "&\\Rightarrow& \\sigma = \\frac{1}{3}, \\text{ when } \\sigma \\in (0,1)\n",
    "\\end{eqnarray*}$$  \n",
    "Which means if we choose a $\\mathbf w$ which makes $\\sigma(\\mathbf w \\cdot \\phi(x)) = \\frac{1}{3}$, we can reach a maximum gradient magnitude.   \n",
    "The max magnitude is $f(1/3) = \\frac{8}{27} \\lVert \\phi(x) \\lVert$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.e\n",
    "Since $\\mathbf w$ makes makes no error on dataset $\\mathbf D$, and it uses non-linear predicator as described above; we have for each data point in $\\mathbf D$:\n",
    "$$\n",
    "y = \\sigma(\\mathbf w \\cdot \\phi(x)) \\Rightarrow \\mathbf w \\cdot \\phi(x) = \\log \\frac{y}{1-y} \n",
    "$$\n",
    "\n",
    "For the transformed $y'$ and least squares regression loss, set the loss to 0 we have:\n",
    "$$\\begin{eqnarray*}\n",
    "L(x, y', \\mathbf w^*) &=& \\frac{1}{2}(\\mathbf w^* \\cdot \\phi(x) - y')^2 \\\\\n",
    "&\\Rightarrow& \\mathbf w^* \\cdot \\phi(x) = y'\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Compare this with the above loss of dataset $\\mathbf D$, it's obvious that if the new dataset $\\mathbf D'$ of ($x,y'$) where $y' = \\log \\frac{y}{1-y}$ when perform least squares regression will also converge to $\\mathbf w^* = \\mathbf w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.f\n",
    "Find the n-gram length that produces errors nearly as small as word features.\n",
    "The function and result is shown below. It can be seen that when n is with [4,7] the training error is almost 0, and the dev error is very close to word feature result (train error = 0.027293190771, dev error = 0.270399549803).  \n",
    "The reason is because on average the in the word features there are around [4,7] letters.   \n",
    "\n",
    "#### Construct a review (one sentence max) in which character n-grams probably outperform word features, and briefly explain why this is so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.f code\n",
    "def test3f():\n",
    "    trainExamples = readExamples('polarity.train')\n",
    "    devExamples = readExamples('polarity.dev')\n",
    "    for i in range(1,10):\n",
    "        featureExtractor = submission.extractCharacterFeatures(i)\n",
    "        weights = submission.learnPredictor(trainExamples, devExamples, featureExtractor, numIters=20, eta=0.01)\n",
    "        trainError = evaluatePredictor(trainExamples, lambda(x) : (1 if dotProduct(featureExtractor(x), weights) >= 0 else -1))\n",
    "        devError = evaluatePredictor(devExamples, lambda(x) : (1 if dotProduct(featureExtractor(x), weights) >= 0 else -1))\n",
    "        print \"%d-gram: train error = %5f, dev error = %5f\" % (i,trainError, devError)\n",
    "\n",
    "The result is:\n",
    "1-gram: train error = 0.458638, dev error = 0.484524\n",
    "2-gram: train error = 0.314575, dev error = 0.414744\n",
    "3-gram: train error = 0.002532, dev error = 0.320484\n",
    "4-gram: train error = 0.000000, dev error = 0.277715\n",
    "5-gram: train error = 0.000000, dev error = 0.274057\n",
    "6-gram: train error = 0.000000, dev error = 0.272651\n",
    "7-gram: train error = 0.000281, dev error = 0.270962\n",
    "8-gram: train error = 0.000563, dev error = 0.293191\n",
    "9-gram: train error = 0.000844, dev error = 0.309229"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.a\n",
    "\n",
    "### 1. $\\mu_1 = [2, 3]$ and $\\mu_2 = [2, -1]$\n",
    "From initial to the second iteration, the centroids and point assignments are shown as:  \n",
    "\n",
    "| iter | Centroid 1 | Point assignment to centroid 1 | Centroid 2 | Point assignment to centroid 2 |\n",
    "|------|------------|--------------|------------|--------------|\n",
    "| 0 | (2,3) | {$\\phi(x_2) = [1, 2], \\phi(x_4) = [2, 2]$} | (2,-1) | {$\\phi(x_1) = [1, 0], \\phi(x_3) = [3, 0]$} |\n",
    "| 1 | (1.5,2) | {$\\phi(x_2) = [1, 2], \\phi(x_4) = [2, 2]$} | (2,0) | {$\\phi(x_1) = [1, 0], \\phi(x_3) = [3, 0]$} |\n",
    "| 2 | (1.5,2) | {$\\phi(x_2) = [1, 2], \\phi(x_4) = [2, 2]$} | (2,0) | {$\\phi(x_1) = [1, 0], \\phi(x_3) = [3, 0]$} |\n",
    "\n",
    "The assignment is hence $z=[2,1,2,1]$, the two centroids are $(1.5,2),(2,0)$\n",
    "\n",
    "### 2. $\\mu_1 = [0, 1]$ and $\\mu_2 = [3, 2]$\n",
    "From initial to the second iteration, the centroids and point assignments are shown as:  \n",
    "\n",
    "| iter | Centroid 1 | Points assignment to centroid 1 | Centroid 2 | Points assignment to centroid 2 |\n",
    "|------|------------|--------------|------------|--------------|\n",
    "| 0 | (0,1) | {$\\phi(x_1) = [1, 0], \\phi(x_2) = [1, 2]$ | (3,2)} | {$\\phi(x_3) = [3, 0],\\phi(x_4) = [2, 2]$} |\n",
    "| 1 | (1,1) | {$\\phi(x_1) = [1, 0], \\phi(x_2) = [1, 2]$ | (2.5,1)} | {$\\phi(x_3) = [3, 0],\\phi(x_4) = [2, 2]$} |\n",
    "| 2 | (1,1) | {$\\phi(x_1) = [1, 0], \\phi(x_2) = [1, 2]$ | (2.5,1)} | {$\\phi(x_3) = [3, 0],\\phi(x_4) = [2, 2]$} |\n",
    "\n",
    "The assignment is hence $z=[1,1,2,2]$, the two centroids are $(1,1),(2.5,1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.d Benefits running k-means with different initializations\n",
    "1. Running k-means multiple times on the same dataset with the same K, but different random initializations can help check the correctnessless of the clustering. \n",
    "2. k-means algorithm is sensitive to initializations, different initialization may have different clustering results. \n",
    "3. For this reason, works have been done to improve the effectiveless of k-means by carefully choosing the starting centroids.\n",
    " * Hierarchical Starting Values, Tseng and Wong (2005)\n",
    " * k-means++, Arthur and Vassilvitskii (2007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.e\n",
    "Applying the same linear transformation on both centroids and data points will not change the final assignment result. For this reason, scaling on all dimensions or only certain dimensions will both result in the same assignment result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
