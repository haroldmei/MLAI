{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1, Nonnegativity. Prove the following:\n",
    "$$\\forall{P,Q}, KL(P \\parallel Q) \\ge 0$$\n",
    "And\n",
    "$$ KL(P||Q) = 0\\ iff\\ P=Q$$\n",
    "\n",
    "Prove 1:\n",
    "$$\\begin{align*}\n",
    "KL(P \\parallel Q)=\\underset{x}{\\Sigma}P(x)log\\frac{P(x)}{Q(x)}\n",
    "&=-\\underset{x}{\\Sigma}P(x)log\\frac{Q(x)}{P(x)} \\\\\n",
    "&>=-log(\\underset{x}{\\Sigma}P(x)\\frac{Q(x)}{P(x)}) \\\\\n",
    "&=-log(\\underset{x}{\\Sigma}{Q(x)}) \\\\\n",
    "&=-log(1) = 0 \\\\\n",
    "\\end{align*}$$\n",
    "\n",
    "Prove 2:\n",
    "$$ KL(P \\parallel Q) = 0 \\text{ if and only if } P = Q$$\n",
    "a. if $P = Q$, $KL(P \\parallel Q) = \\sum 0 = 0$\n",
    "b. if $KL(P\\parallel Q) = 0$, given $-\\log x$ is strictly convex, then $E[-log(\\frac{P}{Q})] = -log(E[\\frac{P}{Q}]) = 0$ with probability 1, so $P = Q$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2, Chain rule for KL divergence (the last step):\n",
    "$$\\begin{align*}\n",
    "KL(P(X) \\parallel Q(X)) + KL(P(Y|X) \\parallel Q(Y|X)) \n",
    "&= \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} + \\sum_x P(x)(\\sum_y P(y|x) \\log \\frac{P(y|x)}{Q(y|x)}) \\\\\n",
    "&= \\sum_x P(x) \\bigg(\\log \\frac{P(x)}{Q(x)} + \\sum_y P(y|x) \\log \\frac{P(y|x)}{Q(y|x)}\\bigg) \\\\\n",
    "&= \\sum_x P(x) \\bigg(\\sum_y P(y|x) \\log \\frac{P(x)}{Q(x)} + \\sum_y P(y|x) \\log \\frac{P(y|x)}{Q(y|x)}\\bigg) \\\\\n",
    "&= \\sum_x P(x) \\bigg(\\sum_y P(y|x) \\bigg(\\log \\frac{P(x)}{Q(x)} + \\log \\frac{P(y|x)}{Q(y|x)}\\bigg) \\bigg) \\\\\n",
    "&= \\sum_x \\bigg(\\sum_y P(y|x)P(x) (\\log \\frac{P(x)P(y|x)}{Q(x)Q(y|x} \\bigg) \\\\\n",
    "&= \\sum_x \\sum_y P(x,y) \\log \\frac{P(x,y)}{Q(x,y)}  \\\\\n",
    "&= KL(P(X, Y) \\parallel Q(X, Y)) \\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3, KL and maximum likelihood:\n",
    "\n",
    "$$\\begin{align*}\n",
    "KL(\\hat P \\parallel P_{\\theta}) \n",
    "&= \\sum_x \\hat P(x) \\log \\frac{\\hat P(x)}{P_{\\theta}(x)} \\\\\n",
    "&= -\\sum_x \\hat P(x) \\log \\frac{P_{\\theta}(x)}{\\hat P(x)} \\\\\n",
    "&= -\\sum_x \\frac{1}{m} \\sum_{i=1}^{m} 1 \\{x^{(i)} = x\\} \\log \\frac{P_{\\theta}(x)}{\\frac{1}{m} \\sum_{i=1}^{m} 1 \\{x^{(i)} = x\\}} \\\\\n",
    "&= - \\frac{1}{m} \\sum_{i=1}^{m} \\log \\frac{P_{\\theta}(x^{(i)})}{\\frac{1}{m} \\sum_{i=1}^{m} } \\\\\n",
    "&= - \\frac{1}{m} \\sum_{i=1}^{m} \\log P_{\\theta}(x^{(i)}) \\\\\n",
    "&= - \\frac{1}{m} \\text{log-likelihood}\n",
    "\\end{align*}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
