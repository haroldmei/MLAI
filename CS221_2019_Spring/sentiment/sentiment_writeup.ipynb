{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS221, Spring 2019, PS2 Sentiment\n",
    "Haiyuan Mei (hmei0411@stanford.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.a\n",
    "* The gradient of hinge loss w.r.t. $\\mathbf{w}$ is \n",
    "  $$\n",
    "    \\nabla_\\mathbf{w} \\text{Loss}_{\\text{hinge}}(x, y, \\mathbf{w}) = \\begin{cases}\n",
    "    \\phi(x) y,  \\mathbf{w} \\cdot \\phi(x) y \\le 1 \\\\\n",
    "    0, \\text{otherwise} \\\\\n",
    "    \\end{cases}\n",
    "  $$\n",
    "  \n",
    "*  Represent the 4 samples with vectors in the order of (\"pretty\", \"good\", \"bad\", \"plot\", \"not\", \"scenery\"), and their margin scores/margins starting from $\\mathbf{w}=\\vec{0}$: \n",
    "\n",
    "  1. ($-1$) pretty bad:  \n",
    "    $x^{(1)}=[1,0,1,0,0,0]$, $y^{(1)}=-1$, $\\mathbf{w}=[0,0,0,0,0,0]$, score=0, margin=0   \n",
    "    $\\nabla_\\mathbf{w} \\text{Loss}_{\\text{hinge}}(x, y, \\mathbf{w})=\\phi(x) y=[-1,0,-1,0,0,0]$  \n",
    "    $\\mathbf{w} = \\mathbf{w} - 0.5 \\times [-1,0,-1,0,0,0] = [0.5,0,0.5,0,0,0]$  \n",
    "    \n",
    "  2. ($+1$) good plot:  \n",
    "    $x^{(2)}=[0,1,0,1,0,0]$, $y^{(2)}=+1$, $\\mathbf{w}=[0.5,0,0.5,0,0,0]$, score=0, margin=0    \n",
    "    $\\nabla_\\mathbf{w} \\text{Loss}_{\\text{hinge}}(x, y, \\mathbf{w})=\\phi(x) y=[0,1,0,1,0,0]$  \n",
    "    $\\mathbf{w} = \\mathbf{w} - 0.5 \\times [0,1,0,1,0,0] = [0.5,0,0.5,0,0,0] - [0,0.5,0,0.5,0,0]=[0.5,-0.5,0.5,-0.5,0,0]$  \n",
    "    \n",
    "  3. ($-1$) not good:  \n",
    "    $x^{(3)}=[0,1,0,0,1,0]$, $y^{(3)}=-1$, $\\mathbf{w}=[0.5,-0.5,0.5,-0.5,0,0]$, score=-0.5, margin=0.5     \n",
    "    $\\nabla_\\mathbf{w} \\text{Loss}_{\\text{hinge}}(x, y, \\mathbf{w})=\\phi(x) y=[0,-1,0,0,-1,0]$  \n",
    "    $\\mathbf{w} = \\mathbf{w} - 0.5 \\times [0,-1,0,0,-1,0] = [0.5,-0.5,0.5,-0.5,0,0] - [0,-0.5,0,0,-0.5,0]=[0.5,0,0.5,-0.5,0.5,0]$  \n",
    "    \n",
    "  4. ($+1$) pretty scenery:  \n",
    "    $x^{(4)}=[1,0,0,0,0,1]$, $y^{(4)}=+1$, $\\mathbf{w}=[0.5,0,0.5,-0.5,0.5,0]$, score=0.5, margin=0.5    \n",
    "    $\\nabla_\\mathbf{w} \\text{Loss}_{\\text{hinge}}(x, y, \\mathbf{w})=\\phi(x) y=[1,0,0,0,0,1]$  \n",
    "    $\\mathbf{w} = \\mathbf{w} - 0.5 \\times [0,-1,0,0,-1,0] = [0.5,0,0.5,-0.5,0.5,0] - [0.5,0,0,0,0,0.5]=[0,0,0.5,-0.5,0.5,-0.5]$ \n",
    "\n",
    "* Conclusion: after the 4 samples trained, the weights for each of the six words are: $[0,0,0.5,-0.5,0.5,-0.5]$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.b\n",
    "Prove that no linear classifier can gain 0 error.\n",
    "* The 4 new datasets are:\n",
    "  1. ($-1$) not good:  \n",
    "    $x^{(1)}=[0,1,0,0,1,0]$, $y^{(1)}=-1$\n",
    "    \n",
    "  2. ($+1$) good:  \n",
    "    $x^{(1)}=[0,1,0,0,0,0]$, $y^{(1)}=+1$\n",
    "    \n",
    "  3. ($+1$) not bad:  \n",
    "    $x^{(1)}=[0,0,1,0,1,0]$, $y^{(1)}=+1$\n",
    "    \n",
    "  4. ($-1$) bad:  \n",
    "    $x^{(1)}=[0,0,1,0,0,0]$, $y^{(1)}=-1$\n",
    "    \n",
    " If there exists a $\\mathbf{w}$ such that it makes no error, then the following should be true:\n",
    " $$\n",
    " \\begin{cases}\n",
    " w_1 + w_4 < 0 \\\\\n",
    " w_1 > 0 \\\\\n",
    " w_2 + w_4 > 0 \\\\\n",
    " w_2 < 0\n",
    " \\end{cases}\n",
    " $$\n",
    " This is impossible, because: if $w_1 > 0$, we must have $w_4<0$; since $w_2 < 0$, then $w_2 + w_4 > 0$ is impossible.\n",
    " \n",
    "* Add an additional feature that could fix the problem, we could add a feature indication whether it is a single word which can make the above inequalities to have possible solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.a\n",
    "The Loss expression can be written as:\n",
    "$$\\begin{eqnarray*}\n",
    "\\text{Loss}(x, y, \\mathbf w) &=& (y - \\sigma(z))^2 \\\\\n",
    " &=& (y - \\sigma(\\mathbf w \\cdot \\phi(x)))^2\\\\\n",
    " &=& \\left ( y - \\frac{1}{1+e^{-\\mathbf w \\cdot \\phi(x)}}\\right ) ^2\n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.b\n",
    "By applying the chain rule of gradient, the gradient w.r.t. $\\mathbf w$:  \n",
    "$$\n",
    "\\nabla_\\mathbf{w} \\text{Loss}(x, y, \\mathbf w) = -2 (y - \\sigma(z))\\sigma(z)(1-\\sigma(z))\\phi(x) \n",
    "$$\n",
    "$\\text{where } z = \\mathbf w \\cdot \\phi(x)$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.C\n",
    "From section b, replace y by 1, think of $||\\phi(x)||$ as some constant and consider the following function:\n",
    "$$\n",
    "f(\\sigma) = 2 (\\sigma-1)^2\\sigma, \\text{where } \\sigma \\in (0,1)\n",
    "$$\n",
    "The function looks like following:\n",
    "![image](./image.png)\n",
    "\n",
    "This function touches 0 twice when $\\sigma=0$ or $\\sigma=1$, but $\\sigma$ can never reach 0 or 1, so f(\\sigma) can not reach a  minimum, it is lower bounded by 0, and it can never be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.d \n",
    "When $\\sigma \\in (0,1)$, it can reach maximun when it's gradient is 0:\n",
    "$$\\begin{eqnarray*}\n",
    "f'(\\sigma) &=& -4 (\\sigma-1)\\sigma - 2(\\sigma-1)^2 \\\\\n",
    "&=& -6\\sigma^2 + 8\\sigma - 2 = 0 \\\\\n",
    "&\\Rightarrow& \\sigma = \\frac{1}{3}, \\text{ when } \\sigma \\in (0,1)\n",
    "\\end{eqnarray*}$$  \n",
    "Which means if we choose a $\\mathbf w$ which makes $\\sigma(\\mathbf w \\cdot \\phi(x)) = \\frac{1}{3}$, we can reach a maximum gradient magnitude.   \n",
    "The max magnitude is $f(1/3) = \\frac{8}{27} \\lVert \\phi(x) \\lVert$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.e\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
