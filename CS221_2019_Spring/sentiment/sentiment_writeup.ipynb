{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS221, Spring 2019, PS2 Sentiment\n",
    "Haiyuan Mei (hmei0411@stanford.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a Stochastic gradient descent, updating the weights\n",
    "* The gradient of hinge loss w.r.t. $\\mathbf{w}$ is \n",
    "  $$\n",
    "    \\nabla_\\mathbf{w} \\text{Loss}_{\\text{hinge}}(x, y, \\mathbf{w}) = \\begin{cases}\n",
    "    -\\phi(x) y,  \\mathbf{w} \\cdot \\phi(x) y \\le 1 \\\\\n",
    "    0, \\text{otherwise} \\\\\n",
    "    \\end{cases}\n",
    "  $$\n",
    "  \n",
    "*  Represent the 4 samples with vectors in the order of (\"pretty\", \"good\", \"bad\", \"plot\", \"not\", \"scenery\"), and their scores/margins starting from $\\mathbf{w}=\\vec{0}$: \n",
    "\n",
    "| - |  x | y | old w | score | margin | gradient | new w |\n",
    "|---|---------|---|---|-------|--------|----------|--------|\n",
    "| 1.|  [1,0,1,0,0,0] | -1 | [0,0,0,0,0,0] | score=0 | margin=0 | [1,0,1,0,0,0]  | [-0.5,0,-0.5,0,0,0] |\n",
    "| 2.|  [0,1,0,1,0,0] | +1 | [-0.5,0,-0.5,0,0,0] | score=0 | margin=0 | [0,-1,0,-1,0,0] | [-0.5,0.5,-0.5,0.5,0,0] |\n",
    "| 3.|  [0,1,0,0,1,0] | -1 | [-0.5,0.5,-0.5,0.5,0,0] | score=0.5 | margin=-0.5 | [0,1,0,0,1,0] | [-0.5,0,-0.5,0.5,-0.5,0] |\n",
    "| 4.|  [1,0,0,0,0,1] | +1 | [-0.5,0,-0.5,0.5,-0.5,0] | score=-0.5 | margin=-0.5 | [-1,0,0,0,0,-1] | [0,0,-0.5,0.5,-0.5,0.5] |\n",
    "\n",
    "* Conclusion: after the 4 samples trained, the weights for each of the six words are: $[0,0,-0.5,0.5,-0.5,0.5]$\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b  Small labeled dataset of four mini-reviews\n",
    "Prove that no linear classifier can gain 0 error. \n",
    "#### 1. The 4 new datasets are:\n",
    "  1. ($-1$) not good:  $x^{(1)}=[1,0,1]$, $y^{(1)}=-1$\n",
    "  2. ($+1$) good:      $x^{(1)}=[1,0,0]$, $y^{(1)}=+1$\n",
    "  3. ($+1$) not bad:   $x^{(1)}=[0,1,1]$, $y^{(1)}=+1$\n",
    "  4. ($-1$) bad:       $x^{(1)}=[0,1,0]$, $y^{(1)}=-1$\n",
    "    \n",
    " If there exists a $\\mathbf{w}$ such that it makes no error, then the following should be true:\n",
    " $$\n",
    " \\begin{cases}\n",
    " w_1 + w_3 < 0 \\\\\n",
    " w_1 > 0 \\\\\n",
    " w_2 + w_3 > 0 \\\\\n",
    " w_2 < 0\n",
    " \\end{cases}\n",
    " $$  \n",
    " \n",
    " This is impossible, because: if $w_1 > 0$, we must have $w_3<0$; since $w_2 < 0$, then $w_2 + w_3 > 0$ is impossible.  \n",
    "\n",
    "\n",
    "#### 2. Add an additional feature that could fix the problem  \n",
    "\n",
    "  * we could add a feature the number of words in the review. Suppose it is the 4th component in the review feature vector; the 4 datasets can be denoted as:\n",
    "    1. ($-1$) not good:  $x^{(1)}=[1,0,1,2]$, $y^{(1)}=-1$\n",
    "    2. ($+1$) good:  $x^{(1)}=[1,0,0,1]$, $y^{(1)}=+1$\n",
    "    3. ($+1$) not bad:  $x^{(1)}=[0,1,1,2]$, $y^{(1)}=+1$\n",
    "    4. ($-1$) bad:  $x^{(1)}=[0,1,0,1]$, $y^{(1)}=-1$\n",
    "  \n",
    "  \n",
    "  * The problem now becomes whether we can find a $\\mathbf w$ which make no error on the dataset for the following inequalities:\n",
    " $$\n",
    " \\begin{cases}\n",
    " w_1 + w_3 + 2w_4 < 0 \\\\\n",
    " w_1 + w_4 > 0 \\\\\n",
    " w_2 + w_3 + 2 w_4 > 0 \\\\\n",
    " w_2 + w_4 < 0\n",
    " \\end{cases}\n",
    " $$  \n",
    " \n",
    " \n",
    "  * Since $\\mathbf w$ can be anything and we can scale it, if we can solve the equation \n",
    "  $$\\begin{bmatrix} 1 & 0 & 1 & 2 \\\\ 1 & 0 & 0 & 1 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 1 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 1 \\\\ 1 \\\\ -1 \\end{bmatrix}$$ \n",
    "  we can find a valid $\\mathbf w$ for the above inqualities.    \n",
    "  \n",
    "  \n",
    "  * Since $ \\begin{bmatrix} 1 & 0 & 1 & 2 \\\\ 1 & 0 & 0 & 1 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 1 & 0 & 1 \\end{bmatrix} $ can be linear transformed to $I$, it can be inversed and the linear equation has solution \n",
    "  $$\\begin{bmatrix} w_1 \\\\ w_2 \\\\ w_3 \\\\ w_4 \\end{bmatrix} = \\begin{bmatrix} -1 \\\\ 1 \\\\ 1 \\\\ -1 \\end{bmatrix} \\begin{bmatrix} 1 & 0 & 1 & 2 \\\\ 1 & 0 & 0 & 1 \\\\ 0 & 1 & 1 & 2 \\\\ 0 & 1 & 0 & 1 \\end{bmatrix} ^{-1} $$\n",
    "  which means the new feature added is enough to fix the above problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a Expression for Loss\n",
    "The Loss expression can be written as:\n",
    "$$\\begin{eqnarray*}\n",
    "\\text{Loss}(x, y, \\mathbf w) &=& (y - \\sigma(z))^2 \\\\\n",
    " &=& (y - \\sigma(\\mathbf w \\cdot \\phi(x)))^2\\\\\n",
    " &=& \\left ( y - \\frac{1}{1+e^{-\\mathbf w \\cdot \\phi(x)}}\\right ) ^2\n",
    "\\end{eqnarray*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.b Compute the gradient of the loss with respect to w\n",
    "\n",
    "By applying the chain rule of gradient, the gradient w.r.t. $\\mathbf w$:  \n",
    "$$\n",
    "\\nabla_\\mathbf{w} \\text{Loss}(x, y, \\mathbf w) = -2 (y - \\sigma(z))\\sigma(z)(1-\\sigma(z))\\phi(x) \n",
    "$$\n",
    "$\\text{where } z = \\mathbf w \\cdot \\phi(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.C Minimum gradient magnitude\n",
    "From section b, replace y by 1, think of $||\\phi(x)||$ as some constant and denote the magnitude as the following function $f(\\sigma)$ (here for the ease of description, the sigmoid function is just denoted by a variable $\\sigma$):\n",
    "$$\\begin{eqnarray*}\n",
    "f(\\sigma) &=& \\lVert -2 (y - \\sigma(z))\\sigma(z)(1-\\sigma(z))\\phi(x) \\rVert \\\\\n",
    "&=& 2 \\lVert \\phi(x) \\rVert (\\sigma-1)^2\\sigma , \\text{where } \\sigma \\in (0,1)\n",
    "\\end{eqnarray*}$$\n",
    "The function looks like following (image generated using google):\n",
    "![image](./image.png)\n",
    "\n",
    "This magnitude function touches 0 twice when $\\sigma=0$ or $\\sigma=1$, but $\\sigma$ can never reach 0 or 1, depending on the sign of $\\mathbf w \\cdot \\phi(x)$, $\\lim_{\\lVert \\mathbf w \\rVert \\longrightarrow \\infty} \\sigma(\\mathbf w \\cdot \\phi(x))=0 \\text{ or } 1$; So $\\lim_{\\lVert \\mathbf w \\rVert \\longrightarrow \\infty} f( \\sigma(\\mathbf w \\cdot \\phi(x))) = 0$ but can never reach a minimum, it is lower bounded by 0, and it can never be 0. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.d Maximum gradient magnitude \n",
    "When $\\sigma \\in (0,1)$, it can reach maximun when it's gradient is 0:\n",
    "$$\\begin{eqnarray*}\n",
    "f'(\\sigma) &=& -4 (\\sigma-1)\\sigma - 2(\\sigma-1)^2 \\\\\n",
    "&=& -6\\sigma^2 + 8\\sigma - 2 = 0 \\\\\n",
    "&\\Rightarrow& \\sigma = \\frac{1}{3}, \\text{ when } \\sigma \\in (0,1)\n",
    "\\end{eqnarray*}$$  \n",
    "Which means if we choose a $\\mathbf w$ which makes $\\sigma(\\mathbf w \\cdot \\phi(x)) = \\frac{1}{3}$, we can reach a maximum gradient magnitude.   \n",
    "The max magnitude is $f(1/3) = \\frac{8}{27} \\lVert \\phi(x) \\lVert$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.e Conversion to least squares regression\n",
    "Since $\\mathbf w$ makes makes no error on dataset $\\mathbf D$, and it uses non-linear predicator as described above; we have for each data point in $\\mathbf D$:\n",
    "$$\n",
    "y = \\sigma(\\mathbf w \\cdot \\phi(x)) \\Rightarrow \\mathbf w \\cdot \\phi(x) = \\log \\frac{y}{1-y} \n",
    "$$\n",
    "\n",
    "For the transformed $y'$ and least squares regression loss, set the loss to 0 we have:\n",
    "$$\\begin{eqnarray*}\n",
    "L(x, y', \\mathbf w^*) &=& \\frac{1}{2}(\\mathbf w^* \\cdot \\phi(x) - y')^2 \\\\\n",
    "&\\Rightarrow& \\mathbf w^* \\cdot \\phi(x) = y'\n",
    "\\end{eqnarray*}$$\n",
    "\n",
    "Compare this with the above loss of dataset $\\mathbf D$, it's obvious that if the new dataset $\\mathbf D'$ of ($x,y'$) where $y' = \\log \\frac{y}{1-y}$ when perform least squares regression will also converge to $\\mathbf w^* = \\mathbf w$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.d One sentence explaination of wrong predictions.\n",
    "1. === home alone goes hollywood , a funny premise until the kids start pulling off stunts not even steven spielberg would know how to do . besides , real movie producers aren't this nice .\n",
    "    * putting too much weight on neutral words such as {start:0.28, spielberg:0.25, etc.}  \n",
    "    \n",
    "    \n",
    "2. === 'it's painful to watch witherspoon's talents wasting away inside unnecessary films like legally blonde and sweet home abomination , i mean , alabama . '\n",
    "    * 'painful' should have large negative weight; 'wasting' should also have a negative weight.  \n",
    "    \n",
    "    \n",
    "3. === wickedly funny , visually engrossing , never boring , this movie challenges us to think about the ways we consume pop culture .\n",
    "    * 'never boring', negative of negative is position, but they both have negative weights; context need to be learned.  \n",
    "    \n",
    "   \n",
    "4. === patchy combination of soap opera , low-tech magic realism and , at times , ploddingly sociological commentary .  \n",
    "    * 'low-tech' should be very negative, and 'magic' and 'realism' seems to be neutral words.  \n",
    "    \n",
    "\n",
    "5. === . . . although this idea is \" new \" the results are tired .\n",
    "    * Putting too much weight on quotation mark. Should exclude punctuation marks from learning.  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.e n-gram character level learning\n",
    "So is splitting the words really necessary or can we just naively consider strings of characters that stretch across words?    \n",
    "The answer is no. The sentiment learning can be just done in character level, such as n-gram features.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.f Compare n-gram feature and word feature\n",
    "* Find the n-gram length that produces errors nearly as small as word features.The function and result is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.f code\n",
    "def test3f():\n",
    "    trainExamples = readExamples('polarity.train')\n",
    "    devExamples = readExamples('polarity.dev')\n",
    "    for i in range(1,10):\n",
    "        featureExtractor = submission.extractCharacterFeatures(i)\n",
    "        weights = submission.learnPredictor(trainExamples, devExamples, featureExtractor, numIters=20, eta=0.01)\n",
    "        trainError = evaluatePredictor(trainExamples, lambda(x) : (1 if dotProduct(featureExtractor(x), weights) >= 0 else -1))\n",
    "        devError = evaluatePredictor(devExamples, lambda(x) : (1 if dotProduct(featureExtractor(x), weights) >= 0 else -1))\n",
    "        print \"%d-gram: train error = %5f, dev error = %5f\" % (i,trainError, devError)\n",
    "\n",
    "The result is:\n",
    "1-gram: train error = 0.458638, dev error = 0.484524\n",
    "2-gram: train error = 0.314575, dev error = 0.414744\n",
    "3-gram: train error = 0.002532, dev error = 0.320484\n",
    "4-gram: train error = 0.000000, dev error = 0.277715\n",
    "5-gram: train error = 0.000000, dev error = 0.274057\n",
    "6-gram: train error = 0.000000, dev error = 0.272651\n",
    "7-gram: train error = 0.000281, dev error = 0.270962\n",
    "8-gram: train error = 0.000563, dev error = 0.293191\n",
    "9-gram: train error = 0.000844, dev error = 0.309229"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It can be seen that when n is with [4,7] the training error is almost 0, and the dev error is very close to word feature result (train error = 0.027293190771, dev error = 0.270399549803). The reason is probably because on average most of the words have around 4~7 letters.   \n",
    "\n",
    "\n",
    "* Construct a review (one sentence max) in which character n-grams probably outperform word features, and briefly explain why this is so.\n",
    "    * 'not bad': n-gram can perform better since word feature would learn two negative weight, but 6-gram can just use one feature for 'notbad' and consider it positive.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a Run 2-means on this dataset until convergence. \n",
    "\n",
    "#### 1. $\\mu_1 = [2, 3]$ and $\\mu_2 = [2, -1]$\n",
    "From initial to the second iteration, the centroids and point assignments are shown as:  \n",
    "\n",
    "| iter | Centroid 1 | Point assignment to centroid 1 | Centroid 2 | Point assignment to centroid 2 |\n",
    "|------|------------|--------------|------------|--------------|\n",
    "| 0 | (2,3) | {$\\phi(x_2) = [1, 2], \\phi(x_4) = [2, 2]$} | (2,-1) | {$\\phi(x_1) = [1, 0], \\phi(x_3) = [3, 0]$} |\n",
    "| 1 | (1.5,2) | {$\\phi(x_2) = [1, 2], \\phi(x_4) = [2, 2]$} | (2,0) | {$\\phi(x_1) = [1, 0], \\phi(x_3) = [3, 0]$} |\n",
    "| 2 | (1.5,2) | {$\\phi(x_2) = [1, 2], \\phi(x_4) = [2, 2]$} | (2,0) | {$\\phi(x_1) = [1, 0], \\phi(x_3) = [3, 0]$} |\n",
    "\n",
    "The assignment is hence $z=[2,1,2,1]$, the two centroids are $(1.5,2),(2,0)$\n",
    "\n",
    "#### 2. $\\mu_1 = [0, 1]$ and $\\mu_2 = [3, 2]$\n",
    "From initial to the second iteration, the centroids and point assignments are shown as:  \n",
    "\n",
    "| iter | Centroid 1 | Points assignment to centroid 1 | Centroid 2 | Points assignment to centroid 2 |\n",
    "|------|------------|--------------|------------|--------------|\n",
    "| 0 | (0,1) | {$\\phi(x_1) = [1, 0], \\phi(x_2) = [1, 2]$ | (3,2)} | {$\\phi(x_3) = [3, 0],\\phi(x_4) = [2, 2]$} |\n",
    "| 1 | (1,1) | {$\\phi(x_1) = [1, 0], \\phi(x_2) = [1, 2]$ | (2.5,1)} | {$\\phi(x_3) = [3, 0],\\phi(x_4) = [2, 2]$} |\n",
    "| 2 | (1,1) | {$\\phi(x_1) = [1, 0], \\phi(x_2) = [1, 2]$ | (2.5,1)} | {$\\phi(x_3) = [3, 0],\\phi(x_4) = [2, 2]$} |\n",
    "\n",
    "The assignment is hence $z=[1,1,2,2]$, the two centroids are $(1,1),(2.5,1)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.c K-means with prior knowledge\n",
    "The prior knowledge can help to decide at least partly the initial centroids, it will help making the algorithm more stable;  \n",
    "The algorithm can as such modified to the following steps:\n",
    "1. Initialize some centroids according to prior knowledge (fixed point);\n",
    "2. Draw the remaining random centroids from all unfixed points;\n",
    "3. Preprocess unfixed points (L2 norm for each point which will be used to calculate distance to centroids.);\n",
    "4. Loop until converge:\n",
    "    1. Calculate distance to every unfixed points, record the shortest centroids and assign a centroid;\n",
    "    2. recalculate new centroids according to current assignments (including both fixed and unfixed points);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.d Benefits running k-means with different initializations\n",
    "1. Running k-means multiple times on the same dataset with the same K, but different random initializations can help check the correctnessless of the clustering. \n",
    "2. k-means algorithm is sensitive to initializations, different initialization may have different clustering results. \n",
    "3. For this reason, works have been done to improve the effectiveless of k-means by carefully choosing the starting centroids.\n",
    " * Hierarchical Starting Values, Tseng and Wong (2005)\n",
    " * k-means++, Arthur and Vassilvitskii (2007)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.e Does scaling of features matter?\n",
    "Applying the same linear transformation on both centroids and data points will not change the final assignment result. For this reason, scaling on all dimensions or only certain dimensions will both result in the same assignment result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
