{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS224N 2020 Winter P4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NMT Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.g Use of masks\n",
    "* what effect the masks have on the entire attention computation:   \n",
    "\n",
    "  In attention computation, $e_{t,i}=(h_t^{dec})^T W_{attProj}h_i^{enc}$ will produce a T dimentional vector, with the ith component corresponds to the ith word in a sentence. The mask vector 'enc_masks' will put a '1' for each padded position, and it's embedding e_t is set to -inf where enc_masks has 1. It has the effect of having an attention 0 for each padded position.  \n",
    "  \n",
    "  \n",
    "\n",
    "* why it is necessary to use the masks in this way:  \n",
    "  \n",
    "  Apart from achieving 0 attention for the padded position, the necessarty of using masks in this way is primarily because we use batch gradient descent during training, and by making each sentence the same length with padded words and masks the training can be done by GPU parallel computation. (Another thoughts of improving parallelization is to sample sentences with similar lengths into batches, hence reducing the number of padded position and improves the efficiency of training.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.i BLEU output\n",
    "Corpus BLEU: 35.706406706846536"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.j Attention comparison\n",
    "* advantage and disadvantage of dot product attention compared to multiplicative attention  \n",
    "  Dot product attention $e_t=(h_t^{dec})^T h^{enc}$ has the a much smaller amount of parameters because it has no attention projection matrix $W_{attProj}$.  \n",
    "  The limitation of Dot product attention is that embeddings in both encoder and decoder need to have the same dimension, which is not required in multiplication attention.   \n",
    "  \n",
    "  \n",
    "* advantage and disadvantage of additive attention compared to multiplicative attention   \n",
    "  Additive attention has better performance for larger dimensions; the hyper parameter can be adjusted in order to avoid high dimension computation, for example the dimension of $v^T$ can be chosen a smaller number so as to achieve a better performance. \n",
    "  \n",
    "  In the case of smaller hidden state dimensionality, additive attention has a much more complex training model because it not only brings non-linearity into attention computation, but also has more training parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyzing NMT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a\n",
    "1. Here’s another favorite of my favorites, “The Starry Night”.\n",
    "  * Problem: grammar issue, failed to use substitute word 'one'.\n",
    "  * Reason: low-resource language pairs, the given target corpus isn't big enough to come up with a good language model  \n",
    "  * Solution:  we can increase the size of the target language corpus so as to increase the quality of the target language model; or use a separate language model task and apply the learned target embeddings to the decoder;  \n",
    "  \n",
    "  \n",
    "2. You know what I do is write for children, and in fact, I’m probably the author for children, more reading in the U.S.  \n",
    "  * Problem: failed to generate grammarly correct long complex sentence.\n",
    "  * Reason: Problem with maintaining context over long text. Another problem of not having a good language model.\n",
    "  * Solution: we can increase the size of the target language corpus so as to increase the quality of the target language model; or use a separate language model task and apply the learned target embeddings to the decoder; \n",
    "  \n",
    "  \n",
    "3. A friend of mine did that – Richard $\\text{<unk>}$.     \n",
    "  * Problem: failed to translate OOV words.\n",
    "  * Reason: OOV words are missing from the embedding matrix.\n",
    "  * Solution: In this particular case, the OOV problem with people's names can be fixed or mitigated by an NER task then simply copy the OOV name to the destination. Another possible solution is to learn OOV word embeddings from dictionary definitions, such as dict2vec.    \n",
    "  \n",
    "    \n",
    "4. You just have to go back to the apple to see it as an epiphany.  \n",
    "  * Problem: apple\n",
    "  * Reason: domain mismatch , “manzana” has multiple meanings, it can be translated to either 'block' or 'mean depending on context. \n",
    "  * Solution: add more language pairs containing 'manzana' -> 'block' in the corpus.      \n",
    "    \n",
    "    \n",
    "5. She saved my life by letting me go to the bathroom in the women's room.  \n",
    "  * Problem: women.\n",
    "  * Reason: model limitations, translation has bias. In Spanish a lot of the words have different gender forms; but in English most of the words doesn't have different gender forms. \n",
    "  * Solution: One way of mitigating the issue is by using larger corpus during training, or putting a gender modifier in front such as 'female teachers' as in this case.  \n",
    "  \n",
    "  \n",
    "6. That’s over 100,000 acres.  \n",
    "  * Problem: acres.\n",
    "  * Reason: model limitations, Common sense error. Failed to deal with measurement system conversion\n",
    "  * Solution: Include more language pairs containing measurement system conversions in the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b \n",
    "1. Error 1\n",
    "    * src: Tambin necesita -- necesita dignidad, amor y placer. Y es nuestro trabajo proporcionar esas cosas.\n",
    "    * ref: It also needs --  it needs dignity,  love and pleasure,  and it's our job to hand those things out.\n",
    "    * nmt: It also needs -- needs dignity , love and pleasure , and it 's our job to provide those things .\n",
    "    * error:  Subjest missing: 'it needs dignity'\n",
    "    * reason:  Lack of language fluency caused by a good language model\n",
    "    * solution:  Increase the size of corpus, or use a pretrained language model.\n",
    "    \n",
    "\n",
    "2. Error 2\n",
    "    * src:  Le encontramos un lugar, la internamos, y la cuidamos y nos encargamos de su familia, porque era necesario,\n",
    "    * ref:  We found her one, we got her there,  and we took care of her  and watched over her family,  because it was necessary.\n",
    "    * nmt:  We found a place , the $\\text{<unk>}$ , and the $\\text{<unk>}$ , and we take care of her family , because necessary .\n",
    "    * error: \n",
    "      1. OOV word problem; \n",
    "      2. missing subject: because 'it was' necessary.\n",
    "    * reason:  \n",
    "      1. OOV words are missing from the embedding matrix.\n",
    "      2. Lack of language model\n",
    "    * solution: \n",
    "      1. Use dict2vec; \n",
    "      2. Increase language model quality by pretrained language models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.c BLEU score\n",
    "\n",
    "#### 1. BLEU\n",
    "* c1: the love can always do; \n",
    "\n",
    "| 1 gram | Count r1 | Count r2 | max(ri) | min(1-gram) |\n",
    "| -- | -- | -- | -- |  -- | \n",
    "|  the | 0 | 0 | 0 | 0 |\n",
    "| love | 1 | 1 | 1 | 1 |\n",
    "| can  | 1 | 0 | 1 | 1 |\n",
    "| always | 1 | 0 | 1 | 1 |\n",
    "|  do  | 0 | 0 | 0 | 0 |  \n",
    "\n",
    "This gives $p_1 = \\frac{\\sum \\min (\\max \\text{Count}_{ri}, \\text{Count}_c)}{\\sum \\text{Count}_c} = \\frac{3}{5}=0.6 $\n",
    "\n",
    "   \n",
    "      \n",
    "| 2 gram | Count r1 | Count r2 | max(ri) | min(2-gram) |\n",
    "| -- | -- | -- | -- | -- |\n",
    "|  the love | 0 | 0 | 0 | 0 |\n",
    "| love can | 1 | 0 | 1 | 1 |\n",
    "| can always | 1 | 0 | 1 | 1 |\n",
    "| always do | 0 | 0 | 0 | 0 |\n",
    "\n",
    "This gives $p_2 = \\frac{\\sum \\min (\\max \\text{Count}_{ri}, \\text{Count}_c)}{\\sum \\text{Count}_c} = \\frac{2}{4}=0.5$   \n",
    "Since r1 and r2 are of the same lenth difference to c, use r2 in this case to calculate $\\text{BP} = 1$;  \n",
    "Finally the blue for c1 is $\\text{BLEU}_{c1} = e^{0.5 ∗ \\log 0.6 + 0.5 * \\log 0.5} = 0.5477$\n",
    "      \n",
    "      \n",
    "* c2: love can make anything possible; \n",
    "\n",
    "| 1 gram | Count r1 | Count r2 | max(ri) | min(1-gram) |\n",
    "| -- | -- | -- | -- |  -- | \n",
    "| love      | 1 | 1 | 1 | 1 |\n",
    "| can       | 1 | 0 | 1 | 1 |\n",
    "| make      | 0 | 0 | 0 | 0 |\n",
    "| anything  | 0 | 1 | 1 | 1 |\n",
    "| possible  | 0 | 1 | 1 | 1 |  \n",
    "\n",
    "This gives $p_1 = \\frac{\\sum \\min (\\max \\text{Count}_{ri}, \\text{Count}_c)}{\\sum \\text{Count}_c} = \\frac{4}{5}=0.8 $\n",
    "\n",
    "   \n",
    "      \n",
    "| 2 gram | Count r1 | Count r2 | max(ri) | min(2-gram) |\n",
    "| -- | -- | -- | -- | -- |\n",
    "| love can          | 1 | 0 | 1 | 1 |\n",
    "| can make          | 0 | 0 | 0 | 0 |\n",
    "| make anything     | 0 | 0 | 0 | 0 |\n",
    "| anything possible | 0 | 1 | 1 | 1 |\n",
    "\n",
    "This gives $p_2 = \\frac{\\sum \\min (\\max \\text{Count}_{ri}, \\text{Count}_c)}{\\sum \\text{Count}_c} = \\frac{2}{4}=0.5$   \n",
    "Same as above, use r2 in this case to calculate $\\text{BP} = 1$;  \n",
    "Finally the blue for c1 is $\\text{BLEU}_{c1} = e^{0.5 ∗ \\log 0.8 + 0.5 * \\log 0.5} = 0.6325$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.  BLEU w.r.t. r1 only\n",
    "* c1:   \n",
    "$p_1 = \\frac{\\sum \\min (\\max \\text{Count}_{ri}, \\text{Count}_c)}{\\sum \\text{Count}_c} = \\frac{3}{5}=0.6 $  \n",
    "$p_2 = \\frac{\\sum \\min (\\max \\text{Count}_{ri}, \\text{Count}_c)}{\\sum \\text{Count}_c} = \\frac{2}{4}=0.5$   \n",
    "Use len(r1) for BP: $\\text{BP} = 1 - \\exp(1-6/5) = 0.8187$  \n",
    "BLUE: $\\text{BLEU}_{c1} = 0.8187 * e^{0.5 ∗ \\log 0.6 + 0.5 * \\log 0.5} = 0.4484$  \n",
    "\n",
    "\n",
    "* c2:  \n",
    "$p_1 = \\frac{\\sum \\min (\\max \\text{Count}_{ri}, \\text{Count}_c)}{\\sum \\text{Count}_c} = \\frac{2}{5}=0.4 $  \n",
    "$p_2 = \\frac{\\sum \\min (\\max \\text{Count}_{ri}, \\text{Count}_c)}{\\sum \\text{Count}_c} = \\frac{1}{4}=0.25$   \n",
    "Use len(r1) for BP: $\\text{BP} = 1 - \\exp(1-6/5) = 0.8187$  \n",
    "BLUE: $\\text{BLEU}_{c1} = 0.8187 * e^{0.5 ∗ \\log 0.4 + 0.5 * \\log 0.25} = 0.2589$  \n",
    "\n",
    "From the BLEU score, c1 is a better translation in this case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Problem with single reference translation\n",
    "\n",
    "The problem with only a single reference translation is that it may cause poor BLEU score even though the translation is very good. There are many different good translation out there, so a good way to increase n-gram overlap is by using several reference translations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Advantages and disadvantages of BLEU\n",
    "\n",
    "Advantages:\n",
    "1. Easy to understand and cost efficient with minimal human labor;\n",
    "2. Can be used on an on-going basis during system development to test changes\n",
    "\n",
    "Disadvantages:\n",
    "1. Hard to distinguish well between subtle differences.\n",
    "2. A good translation can get a poor BLEU score because it has low n-gram overlap with the human translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
