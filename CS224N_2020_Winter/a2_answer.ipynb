{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2: word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.a  Show that: $ - \\sum_{w \\in \\text{Vocab}} y_w \\log \\hat y_w = -\\log ( \\hat(y)_o ) $, answer should be in one line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Answer}$: The ground truth $y_w$ is a one-hot-vector with only the component w.r.t. the outside word being 1, the only term left is $-\\log ( \\hat(y)_o )$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b Compute the partial derivative of $J_{naive-softmax}(v_c, o, U)$ w.r.t. $v_c$.\n",
    "Write equations with matrices and vectors:\n",
    "$$\n",
    "\\begin{align}\n",
    "J_{naive-softmax}(v_c,o,U) &= -\\log \\hat y_o = -y^T \\cdot \\log(\\hat y)  \\\\\n",
    "\\hat y &= p(\\circ | v_c) = \\frac{\\exp(e)}{\\sum \\exp (e)} \\\\\n",
    "e &= U^T v_c\n",
    "\\end{align}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use chain rule of derivatives: $$\\frac{\\partial J}{\\partial v_c} = \\frac{\\partial J}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial e} \\frac{\\partial e}{\\partial v_c}$$.   \n",
    "For the first part and the last part, apply derivative rules w.r.t. vectors and matrices directly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial \\hat y} &= -y \\circ \\frac{1}{\\hat y}   \\\\\n",
    "\\frac{\\partial e}{\\partial v_c} &= U\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the second part, take the derivative element wise:\n",
    "1. If $i \\ne o$, we have: $$\\frac{\\partial \\hat y_i}{\\partial e_o} = - \\frac{\\exp(e_o) \\exp(e_i)}{ (\\sum_{w=1}^V \\exp(e_w))^2} = \\hat y_i \\hat y_o = \\hat y_i (y_i - \\hat y_o) $$\n",
    "2. If $i = o$, we have:\n",
    "$$\n",
    "\\frac{\\partial \\hat y_o}{\\partial e_o} = \\frac{\\exp(e_o)}{\\sum_{w=1}^V \\exp(e_w)} - \\frac{(\\exp(e_o))^2}{ (\\sum_{w=1}^V \\exp(e_w))^2} = \\hat y_o (y_o - \\hat y_o)\n",
    "$$\n",
    "  \n",
    "3. Combine the above 2:\n",
    "$$\n",
    "\\frac{\\partial \\hat y}{\\partial e} = (y - \\hat y) \\circ \\hat y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all 3 parts together:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\partial J}{\\partial v_c} &= -\\frac{1}{\\hat y} \\circ y \\circ \\hat y \\circ (y  - \\hat y)\\cdot U \\\\\n",
    "&=  (\\hat y  -  y ) \\cdot U\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $v_c$ is a D-dimentional vector, the derivative of J w.r.t. $v_c$ is also a D-dimentional vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.c Compute the partial derivative of $J_{naive-softmax}(v_c, o, U)$ w.r.t. $u_w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the chain rule of derivatives again:\n",
    "$$\n",
    "\\frac{\\partial J}{\\partial U} = \\frac{\\partial J}{\\partial \\hat y} \\frac{\\partial \\hat y}{\\partial e} \\frac{\\partial e}{\\partial U}\n",
    "$$\n",
    "\n",
    "The chain rule of derivatives will have two terms in common compared to 1.b:  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial \\hat y} &= -y \\circ \\frac{1}{\\hat y}   \\\\\n",
    "\\frac{\\partial \\hat y}{\\partial e} &= (y - \\hat y) \\circ \\hat y\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The first two terms are the same, the third term is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial e}{\\partial U} &= \\frac{\\partial (U^T v_c)}{\\partial U} = v_c\n",
    "\\end{align}\n",
    "$$\n",
    "  \n",
    "\n",
    "Combine the above 2 cases and consider the result is a DxD matrix, we have: \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\partial J}{\\partial U} &= -\\frac{1}{\\hat y} \\circ y \\circ \\hat y \\circ (y  - \\hat y) \\otimes v_c \\\\\n",
    "\\frac {\\partial J}{\\partial U} &=  (\\hat y - y) \\otimes v_c\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here $\\otimes$ denotes outer product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.d Derivative of sigmoid function\n",
    "Given: \n",
    "$$\\begin{align}\n",
    "\\sigma(x) &= \\frac {1} {1 + \\exp(-x)} \\\\\n",
    " &= \\frac {\\exp(x)} {1 + \\exp(x)} \\\\\n",
    "  &= \\exp(x)\\frac {1} {1 + \\exp(x)} \n",
    "\\end{align}$$\n",
    "\n",
    "The derivative w.r.t. x is:\n",
    "$$\\begin{align}\n",
    "\\sigma'(x) &= (\\exp(x))'\\frac {1} {1 + \\exp(x)} + \\exp(x)(\\frac {1} {1 + \\exp(x)})'  \\\\\n",
    " &= \\frac {\\exp(x)} {1 + \\exp(x)} + \\exp(x) \\frac {\\exp(x)} {(1 + \\exp(x))^2} \\\\\n",
    " &= \\frac {\\exp(x)} {1 + \\exp(x)} (1 - \\frac {\\exp(x)} {1 + \\exp(x)})  \\\\\n",
    " &= \\sigma(x)(1 - \\sigma(x))\n",
    "\\end{align}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.e Gradient w.r.t center/output word vectors when using negative sampling loss\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "J_{neg-sample}(v_c,o,U) &= -\\log(\\sigma(u_o^T v_c)) - \\sum_{k=1}^K \\log(\\sigma(-u_k^T v_c)) \\\\\n",
    "\\sigma(u_o^Tv_c) &= \\frac{1}{1 + \\exp(-u_o^Tv_c)} \n",
    "\\end{align}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Derivative w.r.t. $v_c$ will contain two terms summed together for the expected and negative each <br>\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J}{\\partial v_c} &= \n",
    "\\frac{\\partial J}{\\partial(-log(\\sigma(u_o^Tv_c)))} \\frac{\\partial(-log(\\sigma(u_o^Tv_c)))}{\\partial u_o^Tv_c} \\frac{\\partial u_o^Tv_c}{\\partial v_c} \\\\\n",
    "&+  \\frac{\\partial J}{\\partial(-\\sum_{k=1}^K log(\\sigma(-u_k^Tv_c)))} \\frac{\\partial(-\\sum_{k=1}^K log(\\sigma(-u_k^Tv_c)))}{\\partial u_o^Tv_c} \\frac{\\partial u_o^Tv_c}{\\partial v_c} \\\\\n",
    "&= -\\frac{1}{\\sigma(u_o^Tv_c)} \\sigma(u_o^Tv_c)(1 - \\sigma(u_o^Tv_c))u_o  \\\\\n",
    "&+ \\sum_{k=1}^K (\\frac{1}{\\sigma(-u_k^Tv_c)} \\sigma(-u_k^Tv_c)(1 - \\sigma(-u_k^Tv_c))u_k) \\\\\n",
    "&= -(1 - \\sigma(u_o^Tv_c))u_o + \\sum_{k=1}^K (1 - \\sigma(-u_k^Tv_c))u_k\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "2. Derivative w.r.t. $u_o$ contains only one term because $o \\notin {1,2,...,K}$  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\partial J}{\\partial u_o} &= -\\frac{1}{\\sigma(u_o^Tv_c)} \\sigma(u_o^Tv_c)(1 - \\sigma(u_o^Tv_c))v_c  \\\\\n",
    "&= -(1 - \\sigma(u_o^Tv_c))v_c \n",
    "\\end{align}  \n",
    "$$\n",
    "  \n",
    "  \n",
    "3. Derivative w.r.t. $u_k$ also contains only one term because $k \\in {1,2,...,K}$ and $k \\ne o$  \n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac {\\partial J}{\\partial u_k} &= \\frac{1}{\\sigma(-u_k^Tv_c)} \\sigma(-u_k^Tv_c)(1 - \\sigma(-u_k^Tv_c))v_c   \\\\\n",
    "&= (1 - \\sigma(-u_k^Tv_c))v_c \n",
    "\\end{align}\n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative sampling is much more effective in that it needs no softmax computation, which requires Vocab vector multiplications while negative sampling only needs K+1 vector multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.f Skip-gram loss gradients\n",
    "1. Gradient w.r.t. $U$:\n",
    "$$\n",
    "\\frac{\\partial J_{skip-gram}}{\\partial U} = \\sum_{-m \\le j \\le m, j\\ne0} \\frac{\\partial J(v_c, w_{t+j}, U)}{\\partial U}\n",
    "$$\n",
    "  \n",
    "2. Gradient w.r.t. $v_c$:   \n",
    "$$\n",
    "\\frac{\\partial J_{skip-gram}}{\\partial v_c} = \\sum_{-m \\le j \\le m, j\\ne0}\\frac{\\partial J(v_c, w_{t+j}, U)}{\\partial v_c}\n",
    "$$\n",
    "  \n",
    "3. Gradient w.r.t. $v_w$:\n",
    "$$\n",
    "\\frac{\\partial J_{skip-gram}}{\\partial v_w} = \\sum_{-m \\le j \\le m, j\\ne0}\\frac{\\partial J(v_c, w_{t+j}, U)}{\\partial v_w} = \\vec 0\n",
    "$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
