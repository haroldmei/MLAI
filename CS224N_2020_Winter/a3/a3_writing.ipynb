{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a Adam Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Momentum\n",
    "\n",
    "\n",
    "Standard SGD have problem navigating around ravines. For example, in a 2-D setting, the gradient oscillates on Y direction but keep increasing to the X direction, like the following, on Y direction the gradient keeps oscillating from a large positive value to a large negative value, but on X direction it decreases it's magnitude but keeps moving slowly to the local minima.\n",
    "\n",
    "By applying momentum, gradient changes to the same direction will keep increasing, which achieves faster convergence to the local minima; gradient changes in different directions will have it's magnitude decreased, which achieves reduced update oscillation. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Adaptive learning rate\n",
    "Since the update step is divided by the gradient magnitude in each direction, directions with higher gradient magnitude will have a smaller update rate; directions with lower gradient magnitude will receive higher update rate.  \n",
    "As learning is getting closer to the optima, changes in each direction will get smaller and smaller, hence the convergence will become much slower. Adaptive learning rate will scale up the update steps as the learning reaches to the optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.b Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Value of $\\gamma$\n",
    "The derivation of $\\gamma$:  \n",
    "Simplify $\\mathbb{E}_{P_{drop}}[h_{drop}]_i$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_{P_{drop}}[h_{drop}]_i &= \\mathbb{E}_{P_{drop}}[\\gamma d_i h_i] \\\\\n",
    "&= \\gamma h_i \\mathbb{E}_{P_{drop}}[d_i] \\\\\n",
    "&= \\gamma h_i (1 - P_{drop}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The last equation holds by applying the defionition of dropouts: $d_i$ is 1 with probability of $1 - P_{drop}$. \n",
    "\n",
    "The following equation holds:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\gamma h_i (1 - P_{drop}) &= h_i \\\\\n",
    "& \\Rightarrow \\gamma = \\frac{1}{1 - P_{drop}}\n",
    "\\end{align}\n",
    "$$\n",
    "So the chosen $\\gamma$ should be $\\gamma = \\frac{1}{1 - P_{drop}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Why not dropout during evaluation\n",
    "In deep learning, Dropout is a way to prevent over-fitting. There is no such problem in evaluation so the need for Dropout simply does not exist. Instead, all the activations are used but are reduced by a factor $P_{drop}$ to account for the missing activations during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
