\begin{answer}

The hessian derivation
Take the derivative w.r.t. $\theta_j$,  $h'(x) = g'(\theta^T x) = g(\theta^T x)(1-g(\theta^T x) x_j$ for the jth component.
\begin{eqnarray*}
    \frac{\partial J}{\partial \theta_j}
	&=& -\frac{1}{m} \sum_{i=1}^m \left(y^{(i)} \frac{h_{\theta}(x^{(i)}) (1-h_{\theta}(x^{(i)})} {h_{\theta}(x^{(i)}} x_j
	+ (1 - y^{(i)}) \frac{-h_{\theta}(x^{(i)})  (1-h_{\theta}(x^{(i)}) } {h_{\theta}(x^{(i)}} x_j \right) \\
	&=& -\frac{1}{m} \sum_{i=1}^m \left(y^{(i)}  (1-h_{\theta}(x^{(i)}) x_j
	- (1 - y^{(i)}) h_{\theta}(x^{(i)}) x_j \right) \\
	&=& -\frac{1}{m} \sum_{i=1}^m (y^{(i)} - h_{\theta}(x^{(i)})) x_j
\end{eqnarray*}

The Hessian is just the second order derivative of the cost. Observe the above $\frac{\partial J}{\partial \theta_j}$, 
only the $ h_{\theta}(x^{(i)}) x_j$ part is relevant.
\begin{eqnarray*}
    \frac{\partial^2 J}{\partial \theta_j \theta_k}
	&=& \frac{1}{m} \sum_{i=1}^m  h_{\theta}(x^{(i)})(1 -  h_{\theta}(x^{(i)})) x_j x_k \\
	&=& H_{jk}
\end{eqnarray*}
The diagonals where $j = k$ are also generalized by $H_{jk}$.

Proof of Hessian's PSD.

For any vector $z$, and any given sample $x^{(i)}$, 
consider only $H^{(i)}_{jk}=h_{\theta}(x^{(i)})(1 -  h_{\theta}(x^{(i)})) x_j x_k$:
\begin{eqnarray*}
z^T H^{(i)} z 
	&=& h_{\theta}(x^{(i)})(1 -  h_{\theta}(x^{(i)})) \sum_{j=1}^n \sum_{k=1}^n z_j x_j x_k z_k \\
	&=& h_{\theta}(x^{(i)})(1 -  h_{\theta}(x^{(i)})) (z^T x)^2 \\
	&\ge& 0
\end{eqnarray*}
The last step is true because the sigmoid function can only be (0,1). So $H^{(i)}$ is PSD.

The Hessian H is just the mean of all  $H^{(i)}$, so it is still a Hessian. 

Proved.

\end{answer}
