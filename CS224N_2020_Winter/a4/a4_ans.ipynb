{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS224N 2020 Winter P4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NMT Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.g Use of masks\n",
    "* what effect the masks have on the entire attention computation:   \n",
    "\n",
    "  In attention computation, $e_{t,i}=(h_t^{dec})^T W_{attProj}h_i^{enc}$ will produce a T dimentional vector, with the ith component corresponds to the ith word in a sentence. The mask vector 'enc_masks' will put a '1' for each padded position, and it's embedding e_t is set to -inf where enc_masks has 1. It has the effect of having an attention 0 for each padded position.  \n",
    "  \n",
    "  \n",
    "\n",
    "* why it is necessary to use the masks in this way:  \n",
    "  \n",
    "  Apart from achieving 0 attention for the padded position, the necessarty of using masks in this way is primarily because we use batch gradient descent during training, and by making each sentence the same length with padded words and masks the training can be done by GPU parallel computation. (Another thoughts of improving parallelization is to sample sentences with similar lengths into batches, hence reducing the number of padded position and improves the efficiency of training.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.i BLEU output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.j Attention comparison\n",
    "* advantage and disadvantage of dot product attention compared to multiplicative attention\n",
    "  Dot product attention $e_t=(h_t^{dec})^T h^{enc}$ has the a much smaller amount of parameters because it has no attention projection matrix $W_{attProj}$.  \n",
    "  The limitation of Dot product attention is that embeddings in both encoder and decoder need to have the same dimension, which is not required in multiplication attention.   \n",
    "  \n",
    "  \n",
    "* advantage and disadvantage of additive attention compared to multiplicative attention   \n",
    "  Additive attention has better performance for larger dimensions; the hyper parameter can be adjusted in order to avoid high dimension computation, for example the dimension of $v^T$ can be chosen a smaller number so as to achieve a better performance. \n",
    "  \n",
    "  In the case of smaller hidden state dimensionality, additive attention has a much more complex training model because it brings non-linearity into attention computation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analyzing NMT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a\n",
    "1. Here’s another favorite of my favorites, “The Starry Night”.\n",
    "  * Problem:\n",
    "  * Reason:\n",
    "  * Solution:\n",
    "  \n",
    "  \n",
    "2. You know what I do is write for children, and in fact, I’m probably the author for children, more reading in the U.S.  \n",
    "  * Problem:\n",
    "  * Reason:\n",
    "  * Solution:\n",
    "  \n",
    "  \n",
    "3. A friend of mine did that – Richard $\\text{<unk>}$.     \n",
    "  * Problem: failed to translate OOV words.\n",
    "  * Reason: OOV words are missing from the embedding matrix.\n",
    "  * Solution: In this particular case, the OOV problem with people's names can be fixed or mitigated by an NER task then simply copy the OOV name to the destination. Another possible solution is to learn OOV word embeddings from dictionary definitions, such as dict2vec.    \n",
    "  \n",
    "    \n",
    "4. You just have to go back to the apple to see it as an epiphany.  \n",
    "  * Problem: Failed to generate phrase/idioms\n",
    "  * Reason: phrase/idioms are not in the embedding matrix and hence cannot be predicated as a single unit. \n",
    "  * Solution: 'go around the block' is a phrase meaning 'to have a lot of experience'. 'go back to the apple' in english does not make much sense. One way that can mitigate the problem is to also embed phrase/idioms and learning it as a single embedding.      \n",
    "    \n",
    "    \n",
    "5. She saved my life by letting me go to the bathroom in the women's room.  \n",
    "  * Problem: Failed to predict gender info in translation.\n",
    "  * Reason: In Spanish a lot of the words have different masculine or feminine forms; In this case, the NMT translation first failed in predicting the gender of words, and further translates 'profesores' to 'women' which is likely the consequence incorrect gender inference. \n",
    "  * Solution: \n",
    "  \n",
    "  \n",
    "  \n",
    "6. That’s over 100,000 acres.  \n",
    "  * Problem: Failed to deal with different measurement system.\n",
    "  * Reason: \n",
    "  * Solution: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b \n",
    "1. Error 1\n",
    "    * src: \n",
    "    * ref:\n",
    "    * nmt:\n",
    "    * error:\n",
    "    * reason:\n",
    "    * solution:\n",
    "    \n",
    "\n",
    "2. Error 2\n",
    "    * src:\n",
    "    * ref:\n",
    "    * nmt:\n",
    "    * error:\n",
    "    * reason:\n",
    "    * solution:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.c BLEU score\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
