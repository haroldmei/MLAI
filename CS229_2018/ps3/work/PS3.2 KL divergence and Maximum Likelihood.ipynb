{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) Nonnegativity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove the following:\n",
    "$$\\forall{P,Q}, KL(P \\parallel Q) \\ge 0$$\n",
    "And\n",
    "$$ KL(P||Q) = 0 \\iff P=Q$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROOF 1:\n",
    "$$\\begin{align*}\n",
    "KL(P \\parallel Q) \n",
    "&= \\underset{x}{\\Sigma}P(x)log\\frac{P(x)}{Q(x)} \\\\\n",
    "&=-\\underset{x}{\\Sigma}P(x)log\\frac{Q(x)}{P(x)} \\\\\n",
    "&>=-log(\\underset{x}{\\Sigma}P(x)\\frac{Q(x)}{P(x)}) \\\\\n",
    "&=-log(\\underset{x}{\\Sigma}{Q(x)}) \\\\\n",
    "&=-log(1) = 0 \\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROOF 2:\n",
    "$$ KL(P \\parallel Q) = 0 \\iff P = Q$$\n",
    "\n",
    "a. If $P = Q$, $KL(P \\parallel Q) = \\sum_{x\\in X} P \\log \\frac{P}{Q} = \\sum_{x\\in X} P \\log(1) = 0$\n",
    "\n",
    "b. If $KL(P \\parallel Q) = 0$, given $-\\log x$ is strictly convex, then $E[-log(\\frac{P}{Q})] = -log(E[\\frac{P}{Q}]) = 0$ with probability 1, so $P = Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b) Chain rule for KL divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that:\n",
    "$$KL(P(X, Y) \\parallel Q(X, Y)) = KL(P(X) \\parallel Q(X)) + KL(P(Y|X) \\parallel Q(Y|X))$$\n",
    "\n",
    "PROOF:\n",
    "$$\\begin{align*}\n",
    "KL(P(X) \\parallel Q(X)) + KL(P(Y|X) \\parallel Q(Y|X))\n",
    "&= \\sum_x P(x) \\log \\frac{P(x)}{Q(x)} + \\sum_x P(x)(\\sum_y P(y|x) \\log \\frac{P(y|x)}{Q(y|x)}) \\\\\n",
    "&= \\sum_x P(x) \\bigg(\\log \\frac{P(x)}{Q(x)} + \\sum_y P(y|x) \\log \\frac{P(y|x)}{Q(y|x)}\\bigg) \\\\\n",
    "&= \\sum_x P(x) \\bigg(\\sum_y P(y|x) \\log \\frac{P(x)}{Q(x)} + \\sum_y P(y|x) \\log \\frac{P(y|x)}{Q(y|x)}\\bigg) \\\\\n",
    "&= \\sum_x P(x) \\bigg(\\sum_y P(y|x) \\bigg(\\log \\frac{P(x)}{Q(x)} + \\log \\frac{P(y|x)}{Q(y|x)}\\bigg) \\bigg) \\\\\n",
    "&= \\sum_x \\bigg(\\sum_y P(y|x)P(x) (\\log \\frac{P(x)P(y|x)}{Q(x)Q(y|x} \\bigg) \\\\\n",
    "&= \\sum_x \\sum_y P(x,y) \\log \\frac{P(x,y)}{Q(x,y)}  \\\\\n",
    "&= KL(P(X, Y) \\parallel Q(X, Y)) \\\\\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c) KL and maximum likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that \n",
    "$$\\arg \\min_\\theta KL(\\hat P \\parallel P_{\\theta}) = \\arg \\max_\\theta \\sum_{i=1}^{m} \\log P_{\\theta}(x^{(i)}) $$\n",
    "\n",
    "$$\\begin{align*}\n",
    "KL(\\hat P \\parallel P_{\\theta}) \n",
    "&= \\sum_x \\hat P(x) \\log \\frac{\\hat P(x)}{P_{\\theta}(x)} \\\\\n",
    "&= -\\sum_x \\hat P(x) \\log \\frac{P_{\\theta}(x)}{\\hat P(x)} \\\\\n",
    "&= -\\sum_x \\frac{1}{m} \\sum_{i=1}^{m} 1 \\{x^{(i)} = x\\} \\log \\frac{P_{\\theta}(x)}{\\frac{1}{m} \\sum_{i=1}^{m} 1 \\{x^{(i)} = x\\}} \\\\\n",
    "&= - \\frac{1}{m} \\sum_{i=1}^{m} \\log \\frac{P_{\\theta}(x^{(i)})}{\\frac{1}{m} \\sum_{i=1}^{m} } \\\\\n",
    "&= - \\frac{1}{m} \\sum_{i=1}^{m} \\log P_{\\theta}(x^{(i)}) \\\\\n",
    "&= - \\frac{1}{m} \\text{log-likelihood}\n",
    "\\end{align*}$$\n",
    "\n",
    "Which implies that\n",
    "$$\\arg \\min_\\theta KL(\\hat P \\parallel P_{\\theta}) = \\arg \\max_\\theta \\sum_{i=1}^{m} \\log P_{\\theta}(x^{(i)}) $$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
