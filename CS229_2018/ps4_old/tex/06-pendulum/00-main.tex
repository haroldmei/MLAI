\clearpage
\item \points{20} {\bf Reinforcement Learning: The inverted pendulum}

In this problem, you will apply reinforcement learning to automatically
design a policy for a difficult control task, without ever using any 
explicit knowledge of the dynamics of the underlying system.

The problem we will consider is the inverted pendulum or the pole-balancing 
problem.\footnote{The dynamics are adapted from {\tt
    http://www-anw.cs.umass.edu/rlr/domains.html}}

Consider the figure shown. A thin pole is connected via a free hinge to a cart, 
which can move laterally on a smooth table surface. The controller is said to 
have failed if either the angle of the pole deviates by more than a certain
amount from the vertical position (i.e., if the pole falls over), or if the
cart's position goes out of bounds (i.e., if it falls off the end of the table).
Our objective is to develop a controller to balance the pole with these 
constraints, by appropriately having the cart accelerate left and right.

\begin{figure}
  \centering
  \includegraphics[width=6cm]{06-pendulum/cart_pole.eps}
\end{figure}

We have written a simple Python simulator for this problem. The simulation 
proceeds in discrete time cycles (steps). The state of the cart and pole at any time 
is completely characterized by 4 parameters: the cart position $x$, the 
cart velocity $\dot{x}$, the angle of the pole $\theta$ measured as its deviation 
from the vertical position, and the angular velocity of the pole $\dot{\theta}$.  
Since it'd be simpler to
consider reinforcement learning in a discrete state space,
we have approximated the state space by a discretization that maps 
a state vector $(x,\dot{x}, \theta, \dot{\theta})$ into a number
from 0 to {\tt NUM\_STATES - 1}. Your learning algorithm will need to 
deal only with this discretized representation of the states.

At every time step, the controller must choose one of two actions -
push (accelerate) the cart right, or push the cart left.
(To keep the problem simple, there is no {\it do-nothing} action.)
These are represented as actions $1$ and $2$ respectively in the code.  
When the action choice is made, the simulator updates the state parameters 
according to the underlying dynamics, and provides a new discretized state.

We will assume that the reward $R(s)$ is a function of the current state only.
When the pole angle goes beyond a certain limit or when the cart goes
too far out, a negative reward is given, and the system is reinitialized 
randomly. At all other times, the reward is zero.  Your program must learn 
to balance the pole using only the state transitions and rewards observed.


The code for this problem is in \texttt{p6\_control.py}. This 
file can be run to train a model and to plot a learning curve 
at the end.  Read the comments at the top of the file for more details
on the working of the simulation.


To solve the inverted pendulum problem, you will estimate a 
  model (i.e., transition probabilities and rewards) for the underlying 
  MDP, solve Bellman's equations for this estimated MDP to obtain
  a value function, and act greedily with respect to this value function.  

  Briefly, you will maintain a current model of the MDP and a current
  estimate of the value function. Initially, each state has estimated reward zero,
  and the estimated transition probabilities are uniform (equally likely 
  to end up in any other state).

  During the simulation, you must choose actions at each time step 
  according to some current policy.  As the program goes along taking 
  actions, it will gather observations on transitions and rewards,
  which it can use to get a better estimate of the MDP model.
  Since it is inefficient to update the whole estimated MDP after every 
  observation, we will store the state transitions and reward observations 
  each time, and update the model and value function/policy only periodically. 
  Thus, you must maintain counts of the total number of times the 
  transition from state $s_i$ to state $s_j$ using action $a$ has been 
  observed (similarly for the rewards).  Note that the rewards at 
  any state are deterministic, but the state transitions are not because 
  of the discretization of the state space (several different but close
  configurations may map onto the same discretized state).

  Each time a failure occurs (such as if the pole falls over), you should 
  re-estimate the transition probabilities and rewards as the average of 
  the observed values (if any).  Your program must then use value iteration 
  to solve Bellman's equations on the estimated MDP, to get the value function 
  and new optimal policy for the new model.  For value iteration, use a 
  convergence criterion that checks if the maximum absolute change in the 
  value function for a given state on an iteration exceeds some specified tolerance. 

  Finally, assume that the whole learning procedure has converged 
  once several consecutive attempts (defined by the parameter 
  {\tt NO\_LEARNING\_THRESHOLD}) to solve Bellman's
  equation all converge in the first iteration. Intuitively, this
  indicates that the estimated model has stopped changing significantly.

  Please implement the following functions within \texttt{p6\_control.py}:

  \begin{enumerate}

    \item \texttt{initialize\_mdp\_data}
    \item \texttt{choose\_action}
    \item \texttt{update\_mdp\_transition\_counts\_reward\_counts}
    \item \texttt{update\_mdp\_transition\_probs\_reward}
    \item \texttt{update\_mdp\_value}

  \end{enumerate}